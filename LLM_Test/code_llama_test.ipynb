{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "# print(\"token\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\",torch_dtype=torch.float16)\n",
    "# model.to(device)\n",
    "# print(\"model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer(\"###Instruction\\nGive me an IEC 61131-3 structured text program with two alarms represented by the variables xAlarm1 and xAlarm2. With the signal xAllOn all alarms should be set to TRUE. With the signal xAllOff all alarms should be set to FALSE. The signals should be turned off.\\n###Response\\n\", return_tensors=\"pt\", return_token_type_ids=False).to(device)\n",
    "# print(\"generating\")\n",
    "# start = time.time()\n",
    "# tokens = model.generate(\n",
    "#   **inputs,\n",
    "#   max_new_tokens=1024,\n",
    "#   temperature=0.1,\n",
    "#   do_sample=True,\n",
    "# )\n",
    "# end = time.time()\n",
    "\n",
    "# print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "# print(end-start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.set_default_tensor_type(torch.cuda.HalfTensor)\n",
    "conversation_text = '''Give me an IEC 61131-3 structured text program with two alarms represented by the variables xAlarm1 and xAlarm2. With the signal xAllOn all alarms should be set to TRUE. With the signal xAllOff all alarms should be set to FALSE. The signals should be turned off.'''\n",
    "start = time.time()\n",
    "sequences = pipeline(\n",
    "    conversation_text,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=2048,\n",
    ")\n",
    "end = time.time()\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n",
    "print(end-start, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "import time\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"../../../model/codellama-7b-instruct.Q4_K_M.gguf\", model_type=\"llama\", max_new_tokens=512,temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "import time\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"../../../model/platypus2-13b.Q4_K_M.gguf\", model_type=\"llama\", max_new_tokens=128,temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "PROGRAM AlarmControl\n",
      "VAR\n",
      "\txAlarm1 : BOOL;\n",
      "\txAlarm2 : BOOL;\n",
      "\txAllOn : BOOL;\n",
      "\txAllOff : BOOL;\n",
      "END_VAR\n",
      "\n",
      "IF xAllOn THEN\n",
      "\txAlarm1 := TRUE;\n",
      "\txAlarm2 := TRUE;\n",
      "ELSIF xAllOff THEN\n",
      "\txAlarm1 := FALSE;\n",
      "\txAlarm2 := FALSE;\n",
      "END_IF\n",
      "```\n",
      "448.9260392189026 s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "###Instruction: \n",
    "You are an IEC-61131-3 Structured Text Coding assistant. Your job is to assist me by generating Structured Text, with the functionality I give you. You should only output Structured Text and no other programming language.\n",
    "###Functionality:\n",
    "Give me a program with two alarms represented by the variables xAlarm1 and xAlarm2. With the signal xAllOn all alarms should be set to TRUE. With the signal xAllOff all alarms should be set to FALSE. The signals should be turned off.\n",
    "###Response:\n",
    "\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "print(llm(prompt))\n",
    "end = time.time()\n",
    "print(end-start,'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
